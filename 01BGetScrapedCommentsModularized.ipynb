{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa46d84-b126-47ff-89e4-78817114653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import hashlib\n",
    "\n",
    "API_KEY = 'sk-bedceae2ceba437f944db22706354095'\n",
    "\n",
    "\n",
    "COMMENTS_DIR = \"comments_scraped\"\n",
    "CSV_DIR = \"comments_scraped_csv\"\n",
    "PKL_DIR = \"comments_scraped_pkl\"\n",
    "INSIGHTS_DIR = \"insights\"\n",
    "HASH_DIR = \"hashes\"\n",
    "\n",
    "\n",
    "\n",
    "def save_latest_youtube_urls(creator_id, num_videos=15, output_dir=\"URLs\"):\n",
    "    \"\"\"\n",
    "    Fetches the latest YouTube video URLs from a given creator's channel\n",
    "    and appends new ones to the existing file, avoiding duplicates.\n",
    "\n",
    "    Args:\n",
    "        creator_id (str): YouTube channel handle (e.g., \"@smiletojannah\")\n",
    "        num_videos (int): Number of latest video URLs to fetch\n",
    "        output_dir (str): Folder to save the CSV file with URLs\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of all URLs saved in the CSV (old + new, unique)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    channel_url = f\"https://www.youtube.com/{creator_id}\"\n",
    "    output_path = os.path.join(output_dir, f\"{creator_id.strip('@')}.csv\")\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"yt-dlp\", \"--dump-json\", \"--flat-playlist\", \"--playlist-end\", str(num_videos), channel_url],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "\n",
    "        video_entries = [json.loads(line) for line in result.stdout.strip().split('\\n')]\n",
    "        new_urls = [f\"https://www.youtube.com/watch?v={entry['id']}\" for entry in video_entries]\n",
    "\n",
    "        # Load existing if available\n",
    "        if os.path.exists(output_path):\n",
    "            existing_df = pd.read_csv(output_path)\n",
    "            all_urls = pd.Series(existing_df['urls'].tolist() + new_urls).drop_duplicates().tolist()\n",
    "        else:\n",
    "            all_urls = new_urls\n",
    "\n",
    "        # Save updated list\n",
    "        pd.DataFrame({'urls': all_urls}).to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Total {len(all_urls)} URLs saved to {output_path}\")        \n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to fetch videos for {creator_id}\")\n",
    "        print(e.stderr)\n",
    "        return []\n",
    "        \n",
    "\n",
    "def fetch_latest_youtube_urls(creator_id, dir_=\"URLs\"):\n",
    "    CHANNEL_URLS = pd.read_csv(f'{dir_}/{creator_id}.csv')['urls']\n",
    "    return CHANNEL_URLS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_hash(top_comments):\n",
    "    joined = '\\n'.join(top_comments).strip()\n",
    "    return hashlib.sha256(joined.encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_saved_hash(video_id):\n",
    "    path = os.path.join(HASH_DIR, f\"{video_id}.hash\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def save_hash(video_id, hash_str):\n",
    "    with open(os.path.join(HASH_DIR, f\"{video_id}.hash\"), \"w\") as f:\n",
    "        f.write(hash_str)\n",
    "\n",
    "os.makedirs(COMMENTS_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "os.makedirs(PKL_DIR, exist_ok=True)\n",
    "os.makedirs(INSIGHTS_DIR, exist_ok=True)\n",
    "os.makedirs(HASH_DIR, exist_ok=True)\n",
    "\n",
    "def get_video_id(url):\n",
    "    return url.split('/watch?v=')[-1]\n",
    "\n",
    "def run_yt_dlp(video_id):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(COMMENTS_DIR, f\"{video_id}_at_{timestamp}\")\n",
    "    command = [\"yt-dlp\", \"--skip-download\", \"--write-comments\", \"--no-warnings\", \"--output\", filepath, f\"https://www.youtube.com/watch?v={video_id}\"]\n",
    "    subprocess.run(command, check=True, text=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def get_latest_files():\n",
    "    files = [f for f in os.listdir(COMMENTS_DIR) if f.endswith(\".info.json\")]\n",
    "    latest = {}\n",
    "    for f in files:\n",
    "        match = re.match(r\"(.+?)_at_(\\d+)_\\d+\\.info\\.json\", f)\n",
    "        if match:\n",
    "            vid, ts_str = match.groups()\n",
    "            ts = datetime.strptime(ts_str, \"%Y%m%d\")\n",
    "            if vid not in latest or ts > latest[vid][0]:\n",
    "                latest[vid] = (ts, f)\n",
    "    return [f for _, f in latest.values()]\n",
    "\n",
    "def parse_comments(json_file):\n",
    "    with open(os.path.join(COMMENTS_DIR, json_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    comments = data['comments']\n",
    "    title = data['title']\n",
    "    uploader = data['uploader_id']\n",
    "    \n",
    "    comment_dict = {}\n",
    "    replies = defaultdict(list)\n",
    "    for c in comments:\n",
    "        if c.get(\"parent\") and c[\"parent\"] != \"root\":\n",
    "            replies[c[\"parent\"]].append({**c, \"is_reply\": True})\n",
    "        else:\n",
    "            comment_dict[c[\"id\"]] = {**c, \"replies\": [], \"is_reply\": False}\n",
    "    for pid, rep in replies.items():\n",
    "        if pid in comment_dict:\n",
    "            comment_dict[pid][\"replies\"] = rep\n",
    "    return list(comment_dict.values()), title, uploader\n",
    "\n",
    "def save_comments(video_id, comments):\n",
    "    rows = []\n",
    "    for c in comments:\n",
    "        rows.append({**c, \"parent_id\": None})\n",
    "        for r in c.get(\"replies\", []):\n",
    "            rows.append({**r, \"parent_id\": c[\"id\"]})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(f\"{CSV_DIR}/{video_id}.csv\", index=False)\n",
    "    with open(f\"{PKL_DIR}/{video_id}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(comments, f)\n",
    "    return df\n",
    "\n",
    "def summarize_comments(api_client, comments, title, uploader_id, video_id):\n",
    "    top_comments = [c['text'] for c in comments if not c['is_reply']][:max(1, int(0.05 * len(comments)))]\n",
    "    comment_hash = compute_hash(top_comments)\n",
    "\n",
    "    # Check if same hash already processed\n",
    "    if get_saved_hash(video_id) == comment_hash:\n",
    "        print(f\"üîÅ Skipping {video_id}: already summarized with same top comments.\")\n",
    "        return\n",
    "\n",
    "    user_prompt = '\\n'.join(top_comments)\n",
    "    system_prompt = \"\"\"\n",
    "    The user will provide a list of YouTube comments. Please analyze all the comments together and generate a single, structured JSON object summarizing the overall qualitative dynamics.\n",
    "    \n",
    "    EXAMPLE INPUT:\n",
    "    [\"Great job! This video really opened my eyes.\", \"What a biased take. Shameful.\", \"üòÇüòÇ you're so clueless it's funny.\", \"Pakistan zindabad!\", \"Link to giveaway üëâ http://spamlink\"]\n",
    "    \n",
    "    EXAMPLE JSON OUTPUT:\n",
    "    {\n",
    "      \"overall_sentiment_distribution\": {\"positive\": 1, \"neutral\": 1, \"negative\": 3},\n",
    "      \"dominant_emotions\": [\"anger\", \"sarcasm\", \"joy\"],\n",
    "      \"toxic_comment_count\": 2,\n",
    "      \"controversy_score\": 0.75,\n",
    "      \"key_topics\": [\"bias in media\", \"nationalism\", \"truth and misinformation\"],\n",
    "      \"frequent_bias_or_group_mentions\": [\"Pakistan\", \"India\", \"YouTube creators\"],\n",
    "      \"sarcasm_detected\": true,\n",
    "      \"languages_detected\": [\"English\", \"Urdu\"],\n",
    "      \"spam_comment_count\": 1,\n",
    "      \"summary\": \"The comment section is emotionally charged with a mix of national pride, strong criticism, and sarcasm. There's significant polarization, and a moderate amount of toxicity and spam.\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    response = api_client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format={'type': 'json_object'}\n",
    "    )\n",
    "\n",
    "    \n",
    "    summary = json.loads(response.choices[0].message.content)\n",
    "    summary['title'] = title\n",
    "    summary['uploader_id'] = uploader_id\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(f\"{INSIGHTS_DIR}/{video_id}_insight1_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save hash\n",
    "    save_hash(video_id, comment_hash)\n",
    "    print(f\"‚úÖ Summarized {video_id}\")\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98192aa9-1ee9-4349-84b1-806fec801554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total 35 URLs saved to URLs/smiletojannah.csv\n"
     ]
    }
   ],
   "source": [
    "creator='@smiletojannah'\n",
    "\n",
    "\n",
    "save_latest_youtube_urls(creator, num_videos=15, output_dir=\"URLs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d564d12f-9170-4d3c-ab19-03bb745c19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     https://www.youtube.com/watch?v=0UtHedAXIkA\n",
      "1     https://www.youtube.com/watch?v=7tqYgfhCwb4\n",
      "2     https://www.youtube.com/watch?v=1suta8hYDxo\n",
      "3     https://www.youtube.com/watch?v=Ojm4C0qwhMM\n",
      "4     https://www.youtube.com/watch?v=ItK0890LP-g\n",
      "5     https://www.youtube.com/watch?v=dIAMoGzeut0\n",
      "6     https://www.youtube.com/watch?v=feMoy5ioaeU\n",
      "7     https://www.youtube.com/watch?v=Slx5ZH5NZBM\n",
      "8     https://www.youtube.com/watch?v=aGkNouIMzFY\n",
      "9     https://www.youtube.com/watch?v=4XaMJrS9DvY\n",
      "10    https://www.youtube.com/watch?v=6xhUFLlG_58\n",
      "11    https://www.youtube.com/watch?v=x84vLClrXHw\n",
      "12    https://www.youtube.com/watch?v=YpZnd1xmP_4\n",
      "13    https://www.youtube.com/watch?v=irE6mn-dDZk\n",
      "14    https://www.youtube.com/watch?v=RPd-Buti7gk\n",
      "15    https://www.youtube.com/watch?v=yn-IiJODZLc\n",
      "16    https://www.youtube.com/watch?v=cwOPw3fBMs4\n",
      "17    https://www.youtube.com/watch?v=JTcydTXWh6A\n",
      "18    https://www.youtube.com/watch?v=IsqAu9bt9tI\n",
      "19    https://www.youtube.com/watch?v=jGSm0wWxhxw\n",
      "20    https://www.youtube.com/watch?v=lDBDgMkhZtE\n",
      "21    https://www.youtube.com/watch?v=mXohKXpZqss\n",
      "22    https://www.youtube.com/watch?v=4lth2pImfaE\n",
      "23    https://www.youtube.com/watch?v=rWsg0NgJjjc\n",
      "24    https://www.youtube.com/watch?v=TLh2DXk149Q\n",
      "25    https://www.youtube.com/watch?v=v5sBssBzRcA\n",
      "26    https://www.youtube.com/watch?v=bWM4zjA2rQ8\n",
      "27    https://www.youtube.com/watch?v=-tJZffvpSYA\n",
      "28    https://www.youtube.com/watch?v=JlkaOMU0fqA\n",
      "29    https://www.youtube.com/watch?v=eLy6EayLUG4\n",
      "30    https://www.youtube.com/watch?v=dXlSmWppK28\n",
      "31    https://www.youtube.com/watch?v=spMkR6x3SyU\n",
      "32    https://www.youtube.com/watch?v=hYxIssnzbC4\n",
      "33    https://www.youtube.com/watch?v=O5fsrS6fGCY\n",
      "34    https://www.youtube.com/watch?v=at1F2OmA154\n",
      "Name: urls, dtype: object\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_URLS=fetch_latest_youtube_urls(creator, dir_=\"URLs\")\n",
    "print(CHANNEL_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cb659-e58e-40d2-ae38-44d80c19b010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=0UtHedAXIkA\n",
      "[youtube] 0UtHedAXIkA: Downloading webpage\n",
      "[youtube] 0UtHedAXIkA: Downloading tv client config\n",
      "[youtube] 0UtHedAXIkA: Downloading tv player API JSON\n",
      "[youtube] 0UtHedAXIkA: Downloading ios player API JSON\n",
      "[youtube] 0UtHedAXIkA: Downloading m3u8 information\n",
      "[youtube] Downloading comment section API JSON\n",
      "[youtube] Downloading ~2053 comments\n",
      "[youtube] Sorting comments by newest first\n",
      "[youtube] Downloading comment API JSON page 1 (0/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (11/~2053)\n",
      "[youtube] Downloading comment API JSON page 2 (33/~2053)\n",
      "[youtube] Downloading comment API JSON page 3 (53/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (61/~2053)\n",
      "[youtube] Downloading comment API JSON page 4 (74/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (81/~2053)\n",
      "[youtube] Downloading comment API JSON page 5 (95/~2053)\n",
      "[youtube] Downloading comment API JSON page 6 (115/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (117/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (122/~2053)\n",
      "[youtube] Downloading comment API JSON page 7 (137/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (153/~2053)\n",
      "[youtube] Downloading comment API JSON page 8 (162/~2053)\n",
      "[youtube] Downloading comment API JSON page 9 (182/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (189/~2053)\n",
      "[youtube] Downloading comment API JSON page 10 (203/~2053)\n",
      "[youtube] Downloading comment API JSON page 11 (223/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (224/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (239/~2053)\n",
      "[youtube] Downloading comment API JSON page 12 (245/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (263/~2053)\n",
      "[youtube] Downloading comment API JSON page 13 (266/~2053)\n",
      "[youtube] Downloading comment API JSON page 14 (286/~2053)\n",
      "[youtube] Downloading comment API JSON page 15 (306/~2053)\n",
      "[youtube] Downloading comment API JSON page 16 (326/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (333/~2053)\n",
      "[youtube] Downloading comment API JSON page 17 (347/~2053)\n",
      "[youtube] Downloading comment API JSON page 18 (367/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (371/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (375/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (381/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (388/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (391/~2053)\n",
      "[youtube] Downloading comment API JSON page 19 (394/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (402/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (404/~2053)\n",
      "[youtube] Downloading comment API JSON page 20 (416/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (421/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (436/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (442/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (452/~2053)\n",
      "[youtube] Downloading comment API JSON page 21 (464/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (475/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (485/~2053)\n",
      "[youtube] Downloading comment API JSON page 22 (495/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (503/~2053)\n",
      "[youtube] Downloading comment API JSON page 23 (516/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (520/~2053)\n",
      "[youtube] Downloading comment API JSON page 24 (537/~2053)\n",
      "[youtube] Downloading comment API JSON page 25 (557/~2053)\n",
      "[youtube] Downloading comment API JSON page 26 (577/~2053)\n",
      "[youtube] Downloading comment API JSON page 27 (597/~2053)\n",
      "[youtube] Downloading comment API JSON page 28 (617/~2053)\n",
      "[youtube] Downloading comment API JSON page 29 (637/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (643/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (653/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (663/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (677/~2053)\n",
      "[youtube] Downloading comment API JSON page 30 (687/~2053)\n",
      "[youtube] Downloading comment API JSON page 31 (707/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (716/~2053)\n",
      "[youtube] Downloading comment API JSON page 32 (729/~2053)\n",
      "[youtube] Downloading comment API JSON page 33 (749/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (756/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (762/~2053)\n",
      "[youtube] Downloading comment API JSON page 34 (772/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (776/~2053)\n",
      "[youtube] Downloading comment API JSON page 35 (793/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (794/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (799/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (803/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (812/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (817/~2053)\n",
      "[youtube] Downloading comment API JSON page 36 (818/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (824/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (835/~2053)\n",
      "[youtube] Downloading comment API JSON page 37 (840/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (843/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (847/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (857/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (866/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (873/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (881/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (883/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 7 (888/~2053)\n",
      "[youtube] Downloading comment API JSON page 38 (889/~2053)\n",
      "[youtube] Downloading comment API JSON page 39 (909/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (925/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (930/~2053)\n",
      "[youtube] Downloading comment API JSON page 40 (934/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (950/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (954/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (961/~2053)\n",
      "[youtube] Downloading comment API JSON page 41 (963/~2053)\n",
      "[youtube] Downloading comment API JSON page 42 (983/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (995/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1000/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1009/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1019/~2053)\n",
      "[youtube] Downloading comment API JSON page 43 (1023/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1034/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1039/~2053)\n",
      "[youtube] Downloading comment API JSON page 44 (1049/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1051/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1055/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1059/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1067/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1070/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1072/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 7 (1078/~2053)\n",
      "[youtube] Downloading comment API JSON page 45 (1082/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1084/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1094/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1112/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1122/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1128/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1131/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1141/~2053)\n",
      "[youtube] Downloading comment API JSON page 46 (1153/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1155/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1167/~2053)\n",
      "[youtube] Downloading comment API JSON page 47 (1181/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1189/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1192/~2053)\n",
      "[youtube] Downloading comment API JSON page 48 (1203/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1204/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1210/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1212/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1216/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1220/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1225/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1235/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 7 (1238/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 8 (1244/~2053)\n",
      "[youtube] Downloading comment API JSON page 49 (1245/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1247/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1254/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1257/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1267/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1282/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1292/~2053)\n",
      "[youtube] Downloading comment API JSON page 50 (1311/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1313/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1323/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1337/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1344/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1346/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1356/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1367/~2053)\n",
      "[youtube] Downloading comment API JSON page 51 (1370/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1371/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1381/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1385/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1395/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1425/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1427/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1430/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1434/~2053)\n",
      "[youtube] Downloading comment API JSON page 52 (1444/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1445/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1457/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1467/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 2 (1517/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1529/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1539/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1542/~2053)\n",
      "[youtube] Downloading comment API JSON page 53 (1545/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1546/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1549/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1556/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1560/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1563/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1566/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 7 (1570/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 8 (1578/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 9 (1582/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 10 (1588/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 11 (1596/~2053)\n",
      "[youtube] Downloading comment API JSON page 54 (1601/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1604/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1608/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1616/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1626/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1660/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1663/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1673/~2053)\n",
      "[youtube] Downloading comment API JSON page 55 (1711/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1712/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1722/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 2 (1772/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1792/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1798/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1804/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1808/~2053)\n",
      "[youtube] Downloading comment API JSON page 56 (1823/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1824/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1834/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1842/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1852/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1865/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1873/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1875/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1885/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1915/~2053)\n",
      "[youtube] Downloading comment API JSON page 57 (1920/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (1921/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (1938/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1948/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (1951/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (1955/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (1965/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 5 (1984/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 6 (1987/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 7 (2000/~2053)\n",
      "[youtube] Downloading comment API JSON page 58 (2001/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 1 (2006/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 2 (2017/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 3 (2025/~2053)\n",
      "[youtube]     Downloading comment API JSON reply thread 4 (2032/~2053)\n",
      "[youtube]        Downloading comment replies API JSON page 1 (2042/~2053)\n",
      "[youtube] Extracted 2053 comments\n",
      "[info] 0UtHedAXIkA: Downloading 1 format(s): 399+251\n",
      "[info] Writing video metadata as JSON to: comments_scraped/0UtHedAXIkA_at_20250519_204715.info.json\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=7tqYgfhCwb4\n",
      "[youtube] 7tqYgfhCwb4: Downloading webpage\n",
      "[youtube] 7tqYgfhCwb4: Downloading tv client config\n",
      "[youtube] 7tqYgfhCwb4: Downloading tv player API JSON\n",
      "[youtube] 7tqYgfhCwb4: Downloading ios player API JSON\n"
     ]
    }
   ],
   "source": [
    "for url in CHANNEL_URLS:\n",
    "    video_id = get_video_id(url)\n",
    "    try:\n",
    "        run_yt_dlp(video_id)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed for {video_id}:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ea787-8cbe-434c-8d7d-e7f53227193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "for json_file in get_latest_files():\n",
    "    match = re.match(r\"(.+?)_at_\\d+_\\d+\\.info\\.json\", json_file)\n",
    "    if not match:\n",
    "        continue\n",
    "    video_id = match.group(1)\n",
    "    comments, title, uploader = parse_comments(json_file)\n",
    "    df = save_comments(video_id, comments)\n",
    "    summarize_comments(client, comments, title, uploader, video_id)\n",
    "    print(f\"Completed {video_id}\\n{'*'*10}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f1d4b-b82d-4ca3-a4c3-1676e05d40e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310k",
   "language": "python",
   "name": "py310k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
